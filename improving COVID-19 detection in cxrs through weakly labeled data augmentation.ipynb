{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code base used for evaluating the performance of weakly labeled data augmentation in detecting COVID-19 disease manifestations in CXRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.layers import Add, Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate\n",
    "from keras.layers import SeparableConv2D\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import numpy as np\n",
    "from keras.backend import tensorflow_backend\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import ZeroPadding2D, GlobalAveragePooling2D\n",
    "import time\n",
    "from scipy import interp\n",
    "import cv2\n",
    "import imutils\n",
    "import pickle\n",
    "import struct\n",
    "import shutil\n",
    "import numpy as np\n",
    "import zlib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from sklearn.utils import class_weight\n",
    "from keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "from shutil import copyfile\n",
    "from tensorflow.python.framework import ops\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.losses import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np \n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from PIL import Image\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import backend as keras\n",
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError: # will be 3.x series\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert dicom to png\n",
    "\n",
    "def dcm2png(input_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Becareful all output images are gray image with 8 bit\n",
    "    :param input_dir: dcm file directory\n",
    "    :param output_dir: save directory\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(input_dir):\n",
    "        raise ValueError(\"Input dir is not found!\")\n",
    "\n",
    "    if not os.path.isdir(output_dir):\n",
    "        raise ValueError(\"Out dir is not found!\")\n",
    "\n",
    "    img_list = [f for f in os.listdir(input_dir)\n",
    "                if f.split('.')[-1] == 'dcm' or f.split('.')[-1] == 'jpeg'] \n",
    "    for n, f in enumerate(img_list):\n",
    "        \n",
    "        if f.split(\".\")[-1] == \"dcm\":\n",
    "            dcm_file = input_dir + f\n",
    "            ds = pydicom.dcmread(dcm_file)\n",
    "            pixel_array_numpy = ds.pixel_array\n",
    "            pixel_array_numpy = cv2.normalize(pixel_array_numpy,\n",
    "                                              None,\n",
    "                                              alpha=0,\n",
    "                                              beta=255,\n",
    "                                              norm_type=cv2.NORM_MINMAX,\n",
    "                                              dtype=cv2.CV_8UC1)\n",
    "            pixel_array_numpy = cv2.resize(pixel_array_numpy,\n",
    "                                           (224,224))\n",
    "            img_file = output_dir + f.replace('.dcm', '.png')\n",
    "            cv2.imwrite(img_file, pixel_array_numpy)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            if f.split(\".\")[-1] == \"jpeg\":\n",
    "                image_file = input_dir + f\n",
    "                pixel_array_numpy = cv2.imread(image_file)\n",
    "                pixel_array_numpy = cv2.cvtColor(pixel_array_numpy, cv2.COLOR_BGR2GRAY)\n",
    "                pixel_array_numpy = cv2.resize(pixel_array_numpy,\n",
    "                                               (224,224))\n",
    "                image_file = output_dir + f.replace('.jpeg', '.png')\n",
    "                cv2.imwrite(image_file, pixel_array_numpy)\n",
    "                \n",
    "                if n % 50 == 0:\n",
    "                    print('{} image processed'.format(n))\n",
    "\n",
    "#usage\n",
    "#dcm2png(\"cxr_pneumonia_GT/test/PNEUMONIA/\", \"ped_pneumonia_256/test/PNEUMONIA/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin U-net based semantic segmentation to genrate lung masks for the input CXRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define different loss functions\n",
    "\n",
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (\n",
    "                K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "#another implementation of dice coefficient from https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return dice_loss(y_true, y_pred) + binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "def bce_jac_loss(y_true, y_pred):\n",
    "    return jaccard_distance(y_true, y_pred, smooth=100) + binary_crossentropy(y_true, y_pred)\n",
    "   \n",
    "def iou(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "    \n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def threshold_binarize(x, threshold=0.5):\n",
    "    ge = tf.greater_equal(x, tf.constant(threshold))\n",
    "    y = tf.where(ge, x=tf.ones_like(x), y=tf.zeros_like(x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def iou_thresholded(y_true, y_pred, threshold=0.5, smooth=1.):\n",
    "    y_pred = threshold_binarize(y_pred, threshold)\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Dilated UNET model with dilation rate 2 and dropout of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilatedunet(pretrained_weights = None,input_size = (224,224,1)): #image and masks of size 224x224x1\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)  #empirically determine the best value\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5) \n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = Add()([drop4,up6])\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = Add()([conv3,up7])\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = Add()([conv2,up8])\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', \n",
    "                 dilation_rate=2, kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = Add()([conv1,up9])\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', dilation_rate=2, kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=1e-4), loss=[bce_dice_loss], metrics=[iou, iou_thresholded,'accuracy']) #choose the best loss function\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data format and the functions to train and test with the data using image generators. Make sure to use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same. If you want to visualize the results of generator, set save_to_dir = \"your path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a color dictionary\n",
    "A = [128,128,128]\n",
    "B = [128,0,0]\n",
    "C = [192,192,128]\n",
    "D = [128,64,128]\n",
    "E = [60,40,222]\n",
    "F = [128,128,0]\n",
    "G = [192,128,128]\n",
    "H = [64,64,128]\n",
    "I = [64,0,128]\n",
    "J = [64,64,0]\n",
    "K = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "COLOR_DICT = np.array([A, B, C, D, E, F, G, H, I, J, K, Unlabelled])\n",
    "\n",
    "def adjustData(img,mask,flag_multi_class,num_class):\n",
    "    if(flag_multi_class):\n",
    "        img = img / 255\n",
    "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
    "        new_mask = np.zeros(mask.shape + (num_class,))\n",
    "        for i in range(num_class):\n",
    "            new_mask[mask == i,i] = 1\n",
    "        new_mask = np.reshape(new_mask,(new_mask.shape[0],\n",
    "                                        new_mask.shape[1]*new_mask.shape[2],\n",
    "                                        new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
    "        mask = new_mask\n",
    "    elif(np.max(img) > 1):\n",
    "        img = img / 255\n",
    "        mask = mask /255\n",
    "        mask[mask > 0.5] = 1\n",
    "        mask[mask <= 0.5] = 0\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (224, 224),seed = 1): \n",
    "\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n",
    "        \n",
    "def valGenerator(batch_size,val_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (224, 224),seed = 1): \n",
    "\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    val_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in val_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n",
    "\n",
    "def testGenerator(test_path,target_size = (224,224),flag_multi_class = False,as_gray = True): \n",
    "    for filename in os.listdir(test_path):\n",
    "        img = io.imread(os.path.join(test_path,filename),as_gray = as_gray) \n",
    "        img = img / 255.\n",
    "        img = trans.resize(img,target_size)\n",
    "        img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "        img = np.reshape(img,(1,)+img.shape)\n",
    "        yield img\n",
    "\n",
    "\n",
    "def labelVisualize(num_class,color_dict,img):\n",
    "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i,:] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "\n",
    "def saveResult(save_path,npyfile,test_path, flag_multi_class = False,num_class = 2):\n",
    "    file_names = os.listdir(test_path)\n",
    "    for i,item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        io.imsave(os.path.join(save_path,file_names[i]),img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the dilated UNET model\n",
    "\n",
    "data_gen_args = dict(rotation_range=10.,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=5,\n",
    "                    zoom_range=0.3,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest') \n",
    "myGene = trainGenerator(2,'C:/Users/data/train',\n",
    "                        'image','label',data_gen_args,save_to_dir = None) #batch size of 2 used here\n",
    "\n",
    "\n",
    "model = dilatedunet()\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='loss', patience=15, verbose=1, min_delta=1e-4,\n",
    "                           mode='min'),\n",
    "             ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1,\n",
    "                               epsilon=1e-4, mode='min'),\n",
    "             ModelCheckpoint(monitor='loss', \n",
    "                             filepath='C:/Users/trained_model/dilatedunet.hdf5', \n",
    "                             save_best_only=True,\n",
    "                             mode='min', verbose = 1)]\n",
    "model.fit_generator(generator=myGene,steps_per_epoch=217, epochs=200, callbacks=callbacks,\n",
    "                    verbose=1) #steps_per_epoch=training samples/batchsize + 1 if not absolutely divisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"C:/Users/data/test\" \n",
    "save_path = \"C:/Users/data/membrane/result\" \n",
    "\n",
    "data_gen_args = dict(rotation_range=10.,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=5,\n",
    "                    zoom_range=0.3,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest') \n",
    "\n",
    "testGene = testGenerator(test_path)\n",
    "model = dilatedunet()\n",
    "model.load_weights(\"C:/Users/trained_model/dilatedunet.hdf5\")\n",
    "results = model.predict_generator(testGene,135,verbose=1, workers=1, use_multiprocessing=False) \n",
    "#steps per epoch is the no. of samples in test image.\n",
    "saveResult(save_path, results, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocessing with the mask and image: This script helps to postprocess the images with the mask generated through the UNET and relax the boundaries by 5% on top, bottom, left, and right, and store the bounding box cordinates to a csv file. The cropped bounding box images are stored to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function to generate bounding boxes\n",
    "def generate_bounding_box(image_dir: str, #containing images\n",
    "                          mask_dir: str, #containing masks, images have same name as original images\n",
    "                          dest_csv: str, #CSV file to write the bounding box coordinates\n",
    "                          crop_save_dir: str): #save the cropped bounding box images\n",
    "    \"\"\"\n",
    "    the orginal images are resized to 224x224\n",
    "    the output crops are resized to 224x224\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(mask_dir):\n",
    "        raise ValueError(\"mask_dir not existed\")\n",
    "\n",
    "    case_list = [f for f in os.listdir(mask_dir) if f.split(\".\")[-1] == 'png'] #all mask images are png files\n",
    "\n",
    "    with open(dest_csv, 'w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "\n",
    "        for j, case_name in enumerate(case_list):\n",
    "            mask = cv2.imread(mask_dir + case_name)\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.imread(image_dir + case_name, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (224, 224)) #original images are resized to 224x224\n",
    "            if mask is None or image is None:\n",
    "                raise ValueError(\"The image can not be read: \" + case_name)\n",
    "\n",
    "            reduce_col = np.sum(mask, axis=1)\n",
    "            reduce_row = np.sum(mask, axis=0)\n",
    "            # many 0s add up to none zero, we need to threshold it\n",
    "            reduce_col = (reduce_col >= 255)*reduce_col\n",
    "            reduce_row = (reduce_row >= 255)*reduce_row\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "\n",
    "            last = 0\n",
    "            for i in range(reduce_col.shape[0]):\n",
    "                current = reduce_col[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_col[i]\n",
    "\n",
    "            up = first_none_zero\n",
    "            down = last_none_zero\n",
    "\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "            last = 0\n",
    "            for i in range(reduce_row.shape[0]):\n",
    "                current = reduce_row[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_row[i]\n",
    "\n",
    "            left = first_none_zero\n",
    "            right = last_none_zero\n",
    "\n",
    "            if up is None or down is None or left is None or right is None:\n",
    "                raise ValueError(\"The border is not found: \" + case_name)\n",
    "            \n",
    "            # new coordinates for image which is 1 times of mask, mask images are 224x224, \n",
    "            #so need to multiply 1 times to get 224x224, and relaxing the borders by 5% on all directions\n",
    "            up_down_loose = int(1 * (down - up + 1) * 0.05)\n",
    "            image_up = 1 * up - up_down_loose\n",
    "            if image_up < 0:\n",
    "                image_up = 0\n",
    "            image_down = 1*(down+1)+up_down_loose\n",
    "            if image_down > image.shape[0] + 1:\n",
    "                image_down = image.shape[0]\n",
    "\n",
    "            left_right_loose = int(1 * (right - left) * 0.05)\n",
    "            image_left = 1 * left - left_right_loose\n",
    "            if image_left < 0:\n",
    "                image_left = 0\n",
    "            image_right = 1*(right + 1)+left_right_loose\n",
    "            if image_right > image.shape[1] + 1:\n",
    "                image_right = image.shape[1]\n",
    "\n",
    "            crop = image[image_up: image_down, image_left: image_right]\n",
    "            crop = cv2.resize(crop, (224,224)) #the cropped image is resized to 224x224\n",
    "\n",
    "            cv2.imwrite(crop_save_dir + case_name, crop) # cropped images saved to crop directory\n",
    "\n",
    "            # write new csv\n",
    "            crop_width = image_right - image_left + 1\n",
    "            crop_height = image_down - image_up + 1\n",
    "\n",
    "            csv_writer.writerow([case_name,\n",
    "                                 image_left,\n",
    "                                 image_up,\n",
    "                                 crop_width,\n",
    "                                 crop_height]) #writes xmin, ymin, width, and height\n",
    "\n",
    "            if j % 50 == 0:\n",
    "                print(j, \" images are processed!\")\n",
    "\n",
    "#train-normal\n",
    "generate_bounding_box(\"C:/Users/data/test/\",\n",
    "                      \"C:/Users/result/mask_1/\",\n",
    "                      'C:/Users/result/bounding_box.csv',\n",
    "                      \"C:/Users/result/cropped1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the case if the CXR images have ground truth ROI annotations, we need to downscale and store these values after bounding box cropping. In this regard, we can use the following custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bounding_box(image_dir: str,\n",
    "                          mask_dir: str,\n",
    "                          orgin_csv: str,\n",
    "                          dest_csv: str,\n",
    "                          crop_save_dir: str,\n",
    "                          show_results: bool=False):\n",
    "    if not os.path.isdir(mask_dir):\n",
    "        raise ValueError(\"mask_dir not existed\")\n",
    "\n",
    "    csv_file_data = []\n",
    "    with open(orgin_csv, 'r') as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        for row in reader:\n",
    "            csv_file_data.append(row)\n",
    "\n",
    "    with open(dest_csv, 'w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "\n",
    "        for j, row in enumerate(csv_file_data[1:]):\n",
    "            case_name = row[0] + '.png' #all mask files are png file type\n",
    "            mask = cv2.imread(mask_dir + case_name)\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.imread(image_dir + case_name, cv2.COLOR_BGR2GRAY)\n",
    "            #original images are resized to 256x256, comment this if you want to keep the original image resolution\n",
    "            image = cv2.resize(image, (256,256)) \n",
    "            if mask is None or image is None:\n",
    "                raise ValueError(\"The image can not be read: \" + case_name)\n",
    "            \n",
    "            reduce_col = np.sum(mask, axis=1)\n",
    "            reduce_row = np.sum(mask, axis=0)\n",
    "            # many 0s add up to none zero, we need to threshold it\n",
    "            reduce_col = (reduce_col >= 255)*reduce_col\n",
    "            reduce_row = (reduce_row >= 255)*reduce_row\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "\n",
    "            last = 0\n",
    "            for i in range(reduce_col.shape[0]):\n",
    "                current = reduce_col[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_col[i]\n",
    "\n",
    "            up = first_none_zero\n",
    "            down = last_none_zero\n",
    "\n",
    "            first_none_zero = None\n",
    "            last_none_zero = None\n",
    "            last = 0\n",
    "            for i in range(reduce_row.shape[0]):\n",
    "                current = reduce_row[i]\n",
    "                if last == 0 and current != 0 and first_none_zero is None:\n",
    "                    first_none_zero = i\n",
    "\n",
    "                if current != 0:\n",
    "                    last_none_zero = i\n",
    "\n",
    "                last = reduce_row[i]\n",
    "\n",
    "            left = first_none_zero\n",
    "            right = last_none_zero\n",
    "            \n",
    "            if up is None or down is None or left is None or right is None:\n",
    "                raise ValueError(\"The border is not found: \" + case_name)\n",
    "            \n",
    "            # new coordinates for image which is 1 times of mask, mask images are 256x256, \n",
    "            #need to multiply 1 times to get 256x256, and relaxing the borders by 5% on all directions\n",
    "            \n",
    "            loose = int(1 * (down - up + 1) * 0.05) \n",
    "            # for example, if the original image resolution is \n",
    "            #1024x1024, new coordinates for image is 4 times of mask (256x256), \n",
    "            #need to multiply 4 times to 1024x1024, and relaxing the borders by 5% on all directions, \n",
    "            #i.e. int(4 * (down - up + 1) * 0.05)\n",
    "            image_up = 1 * up - loose #4\n",
    "            if image_up < 0:\n",
    "                image_up = 0\n",
    "            image_down = 1*(down+1)+loose #4\n",
    "            if image_down > image.shape[0] + 1:\n",
    "                image_down = image.shape[0]\n",
    "\n",
    "            loose2 = int(1 * (right - left + 1) * 0.05) #4\n",
    "            image_left = 1 * left - loose2 #4\n",
    "            if image_left < 0:\n",
    "                image_left = 0\n",
    "            image_right = 1*(right+1)+loose2 #4\n",
    "            if image_right > image.shape[1] + 1:\n",
    "                image_right = image.shape[1]\n",
    "\n",
    "            crop = image[image_up: image_down, image_left: image_right]\n",
    "            crop = cv2.resize(crop, (256,256)) #1024, 1024 before\n",
    "\n",
    "            # store images in normal or abnormal folder\n",
    "            if row[6] != 'Normal':\n",
    "                subfolder = 'abnormal/'\n",
    "            else:\n",
    "                subfolder = 'normal/'\n",
    "            cv2.imwrite(crop_save_dir + subfolder + case_name, crop)\n",
    "\n",
    "            # write new csv\n",
    "            crop_width = image_right - image_left + 1\n",
    "            crop_height = image_down - image_up + 1\n",
    "\n",
    "            if row[6] == \"Lung Opacity\":\n",
    "\n",
    "                y_scale_change = 256 / crop_height #1024 before\n",
    "                x_scale_change = 256 / crop_width # 1024 before, image size was 1024 by 1024\n",
    "\n",
    "                bbox_y = int(float(row[2])) \n",
    "                #new_y = int((bbox_y - image_up) * y_scale_change)\n",
    "                new_y = int((bbox_y/4 - image_up) * y_scale_change) #since resized to 256 from 1024, scale by 1/4\n",
    "\n",
    "                bbox_x = int(float(row[1]))\n",
    "                #new_x = int((bbox_x - image_left) * x_scale_change)\n",
    "                new_x = int((bbox_x/4 - image_left) * x_scale_change)  #since resized to 256 from 1024, scale by 1/4\n",
    "\n",
    "                bbox_width = int(float(row[3]))\n",
    "                bbox_height = int(float(row[4]))\n",
    "                #new_width = int(bbox_width/4 * x_scale_change)\n",
    "                #new_height = int(bbox_height/4 * y_scale_change)\n",
    "                new_width = int(bbox_width/4 * x_scale_change)  #since resized to 256 from 1024, scale by 1/4\n",
    "                new_height = int(bbox_height/4 * y_scale_change) #since resized to 256 from 1024, scale by 1/4\n",
    "\n",
    "                csv_writer.writerow([case_name,\n",
    "                                     image_left,\n",
    "                                     image_up,\n",
    "                                     crop_width,\n",
    "                                     crop_height,\n",
    "                                     new_x,\n",
    "                                     new_y,\n",
    "                                     new_width,\n",
    "                                     new_height,\n",
    "                                     row[6]])\n",
    "\n",
    "                else:\n",
    "                csv_writer.writerow([case_name,\n",
    "                                     image_left,\n",
    "                                     image_up,\n",
    "                                     crop_width,\n",
    "                                     crop_height,\n",
    "                                     \"\",\n",
    "                                     \"\",\n",
    "                                     \"\",\n",
    "                                     \"\",\n",
    "                                     row[6]])\n",
    "            \"\"\"\n",
    "            if show_results:\n",
    "                \n",
    "                # image distribution\n",
    "                plt.hist(mask.ravel(), 256, [0, 256])\n",
    "                mask = mask / 255\n",
    "                # Bounding box for image and mask\n",
    "                mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "                rect_mask = cv2.rectangle(mask, (left, up), (right, down), (0, 255, 0), 2)\n",
    "                rect_image = cv2.rectangle(image, (4 * left, 4 * up), (4 * right, 4 * down), (0, 255, 0), 2)\n",
    "                cv2.imshow(\"crop\", crop)\n",
    "                cv2.imshow(\"mask\", rect_mask)\n",
    "                cv2.imshow(\"image\", rect_image)\n",
    "            \"\"\"\n",
    "            if j % 100 == 0:\n",
    "                print(j, \" images are processed!\")\n",
    "\n",
    "\n",
    "generate_bounding_box(\"C:/Users/dcm_2_image/\",\n",
    "                      \"C:/Users/dcm_2_image_mask/\",\n",
    "                      'C:/Users/stage_2_train_labels.csv',\n",
    "                      'C:/Users/bounding_box_rsna_256.csv',\n",
    "                      \"C:/Users/rajaramans2/codes/crops/\",\n",
    "                      show_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tht we have preprocessed the orignal CXRs and cropped them to a size of the bounding box with 224x224 pixel resoultion and grayscale images, we can begin training the models. Initially all models are trained and tested on the pediatric CXR collection to classify them into bacterial and viral pneumonia categories. The best model with these pretrained weights is used to predict the Montreal and Twitter COVID-19 CXR collections as belonging to bacterial or viral pnuemonia cateogry and the results are tabulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% custom function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% path to input data\n",
    "train_data_dir = '.../train'\n",
    "test_data_dir = '.../test'\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "channel = 3\n",
    "epochs = 32\n",
    "batch_size = 8 #vary this parameter depending on your GPU capacity\n",
    "num_classes= 2 #[bacterial, viral]\n",
    "input_shape = (img_width, img_height, channel)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% declared data generators, allocate 20% of the training data for validation\n",
    "\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_data_dir, \n",
    "    target_size=(224,224),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=13,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=13,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(224,224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(val_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(val_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use a wide residual custom CNN for this task. Model architecture taken from https://github.com/titu1994/Wide-Residual-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0005\n",
    "def initial_conv(input):\n",
    "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(input)\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, \n",
    "                           epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def expand_conv(init, base, k, strides=(1, 1)):\n",
    "    x = Convolution2D(base * k, (3, 3), padding='same', \n",
    "                      strides=strides, kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(init)\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, \n",
    "                           epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    skip = Convolution2D(base * k, (1, 1), padding='same', \n",
    "                         strides=strides, kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(init)\n",
    "\n",
    "    m = Add()([x, skip])\n",
    "\n",
    "    return m\n",
    "\n",
    "def conv1_block(input, k=1, dropout=0.0):\n",
    "    init = input\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, \n",
    "                           epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    m = Add()([init, x])\n",
    "    return m\n",
    "\n",
    "def conv2_block(input, k=1, dropout=0.0):\n",
    "    init = input\n",
    "\n",
    "    channel_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, \n",
    "                           epsilon=1e-5, gamma_initializer='uniform')(input)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, \n",
    "                           epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    m = Add()([init, x])\n",
    "    return m\n",
    "\n",
    "def conv3_block(input, k=1, dropout=0.0):\n",
    "    init = input\n",
    "\n",
    "    channel_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, \n",
    "                           epsilon=1e-5, gamma_initializer='uniform')(input)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, \n",
    "                           epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                      W_regularizer=l2(weight_decay),\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    m = Add()([init, x])\n",
    "    return m\n",
    "\n",
    "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    ip = Input(shape=input_dim)\n",
    "    x = initial_conv(ip)\n",
    "    nb_conv = 4\n",
    "    x = expand_conv(x, 16, k)\n",
    "    nb_conv += 2\n",
    "    for i in range(N - 1):\n",
    "        x = conv1_block(x, k, dropout)\n",
    "        nb_conv += 2\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
    "    nb_conv += 2\n",
    "    for i in range(N - 1):\n",
    "        x = conv2_block(x, k, dropout)\n",
    "        nb_conv += 2\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
    "    nb_conv += 2\n",
    "    for i in range(N - 1):\n",
    "        x = conv3_block(x, k, dropout)\n",
    "        nb_conv += 2\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D((8, 8))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n",
    "    model = Model(ip, x, name = 'wide-residual-cnn')\n",
    "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate custom WRN model with a depth of 4 and width of 10, and dropout of 0.3\n",
    "model = create_wide_residual_network(input_shape, nb_classes=2, \n",
    "                                            N=4, k=10, dropout=0.3, verbose=1) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the pretrained model architecture: models used in this study: VGG-16, Inception-V3, Xception, DenseNet-121, and NasNet-mobile. The truncated models are added with a global average pooling and final dense layer and retrained from the scratch. Here, we show the training process with the VGG-16 model. It has to be replaced with the other pretrained models when trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = applications.VGG16(weights='imagenet', \n",
    "                                   include_top=False,\n",
    "                                   input_shape=(img_width,img_height,channel))\n",
    "feature_model = Model(inputs=feature_model.input,\n",
    "                    outputs=feature_model.get_layer('block5_conv3').output)\n",
    "feature1_model.summary()\n",
    "\n",
    "#addind the top layers\n",
    "x = feature1_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(num_classes, activation='softmax', \n",
    "                    name='predictions')(x)\n",
    "model = Model(inputs=feature_model.input, \n",
    "              outputs=predictions, name='vgg16_base_model')\n",
    "model.summary()\n",
    "\n",
    "#enumerate and print layer names\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% fix optimizer and start training the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "#compile the model\n",
    "model.compile(optimizer=sgd,              \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the generators first or it will give weird results\n",
    "train_generator.reset()\n",
    "val_generator.reset()\n",
    "\n",
    "# start training\n",
    "start = time.time()\n",
    "filepath = 'weights/' + model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
    "                             save_weights_only=False, \n",
    "                             save_best_only=True, mode='min', period=1)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "custom_vgg16_history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=nb_train_samples // batch_size + 1, #see if not absolutely divisble by batch size\n",
    "      epochs=epochs,\n",
    "      validation_data=val_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      class_weight = class_weights,\n",
    "      validation_steps=nb_validation_samples // batch_size + 1, \n",
    "      verbose=1)\n",
    "\n",
    "#print the total time taken for training\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot performance\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"vgg16_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate with the best stored model weights\n",
    "model.load_weights('weights/vgg16_base_model.14-0.9154.h5') #change this to your path and model weights\n",
    "model.summary()\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#predict\n",
    "test_generator.reset()\n",
    "custom_vgg16_y_pred = model.predict_generator(test_generator, \n",
    "                                              nb_test_samples/batch_size, workers=1)\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "\n",
    "#print the shape of y_pred and Y_test\n",
    "print(custom_vgg16_y_pred.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#measure accuracy\n",
    "custom_vgg16_model_accuracy=accuracy_score(Y_test,custom_vgg16_y_pred.argmax(axis=-1))\n",
    "print('The accuracy of custom VGG16 model is: ', custom_vgg16_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% #print classification report\n",
    "\n",
    "target_names = ['class 0(bacterial)', 'class 1(viral)'] #from the generator.class_indices\n",
    "print(classification_report(Y_test,\n",
    "                            custom_vgg16_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test, custom_vgg16_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for custom VGG16 model without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store predictions: \n",
    "#predicted_class_indices has the predicted labels, 0 for bacterial and 1 for viral\n",
    "predicted_class_indices=np.argmax(custom_vgg16_y_pred,axis=1)\n",
    "\n",
    "#map the predicted labels with their unique ids \n",
    "#such as filenames to find out what you predicted for which image.\n",
    "labels = (train_generator.class_indices) # 0 for bacterial and 1 for viral\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices] #displays as string\n",
    "\n",
    "#Finally, save the results to a CSV file.\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"predictions_base_vgg16.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test1[:, i], custom_vgg16_y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test1.ravel(), custom_vgg16_y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw = 2\n",
    "fig=plt.figure(figsize=(15,10), dpi=70)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "\n",
    "#plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('multi-class ROC curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model with covid-19 cxr collection from Montreal and Twitter and record the baseline performance: We will also test with the data from other datasets including RSNA, CheXpert, and NIH and weakly label them to bacterial and viral cases and store them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data directory and declare test generators\n",
    "#test_data_dir = 'datasets/cropped_ieee_covid19_04152020'\n",
    "#test_data_dir = 'datasets/cropped_twitter_covid19'\n",
    "#test_data_dir = 'datasets/chexpert_dataset/chexpert_pneumonia'\n",
    "test_data_dir = 'datasets/rsna_dataset/rsna_pneumonia'\n",
    "#test_data_dir = 'datasets/nih_dataset/nih_pneumonia'\n",
    "\n",
    "#declare test generators\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(224,224),\n",
    "        batch_size=1,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load the best model and predict on the test data\n",
    "model.load_weights('weights/vgg16_base_model.h5') # the best performing model from baseline\n",
    "model.summary()\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer=sgd,              \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#predict\n",
    "test_generator.reset()\n",
    "print('-'*30)\n",
    "print('Predicting on the test data...')\n",
    "print('-'*30)\n",
    "y_pred_base = model.predict_generator(test_generator,\n",
    "                                 (nb_test_samples//1),\n",
    "                                 verbose=1)\n",
    "#print prediction shapes\n",
    "print(y_pred_base.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#measure accuracy\n",
    "model_accuracy=accuracy_score(Y_test,y_pred_base.argmax(axis=-1))\n",
    "print('The accuracy of the model is: ', model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% #print classification report\n",
    "target_names = ['Bacterial_pneumonia', 'Viral_pneumonia']\n",
    "print(classification_report(Y_test,y_pred_base.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,y_pred_base.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test,y_pred_base.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(15,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=70)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_class_indices has the predicted labels, 0 for bacterial and 1 for viral\n",
    "predicted_class_indices=np.argmax(y_pred_base,axis=1)\n",
    "#map the predicted labels with their unique ids \n",
    "#such as filenames to find out what you predicted for which image.\n",
    "labels = (train_generator.class_indices) # 0 for bacterial and 1 for viral\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices] #displays as string\n",
    "\n",
    "#Finally, save the results to a CSV file.\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"rsna_vgg16.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV file we will copy the loosely labled CXR images as bacterial or viral pneumonia into separate folders. Here the process is shown with the RSNA pneumonia collection loosely labled into bacterial and viral pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"C:\\\\Users\\\\datasets\\\\rsna_pneumonia\\\\\"\n",
    "dst = \"C:\\\\Users\\\\rsna_bacterial_pneumonia\\\\\"\n",
    "dst1 = \"C:\\\\Users\\\\rsna_viral_pneumonia\\\\\"\n",
    "df = pd.read_csv('rsna_vgg16.csv') \n",
    "df_names = df['Filename'].values\n",
    "df_class = df['Predictions'].values\n",
    "for i in range(len(df_class)):\n",
    "    src_path = os.path.join(src, df_names[i])\n",
    "    if df_class[i] == 'viral':\n",
    "        #dst_path = os.path.join(dst1,df_names[i].split(os.sep)[-1])\n",
    "        dst_path = os.path.join(dst1,df_names[i])\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        #dst_path = os.path.join(dst,df_names[i].split(os.sep)[-1])\n",
    "        dst_path = os.path.join(dst,df_names[i])\n",
    "        shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now augment the baseline train data with the weakly labeled bacterial and viral CXR images from each of the datasets and also on various combinations as mentioned:\n",
    "baseline, baseline+rsna, baseline+nih, baseline+chexpert, baseline+rsna+chexpert, baseline+rsna+nih, baseline+nih+chexpert,\n",
    "baseline+rsna+nih+chexpert\n",
    "The test data will be the baseline test data, Montreal covid19, or Twitter covid-19. we will compare the performance of the baseline model on these three different test data, with the performance achieved by training the models with the aforementioned eight combinations of weak label augmentation with an aim to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the path to data\n",
    "train_data_dir = 'datasets/base_nih/train'\n",
    "#train_data_dir = 'datasets/base_rsna/train'\n",
    "#train_data_dir = 'datasets/base_chexpert/train'\n",
    "#train_data_dir = 'datasets/base_nih_rsna/train'\n",
    "#train_data_dir = 'datasets/base_nih_chexpert/train'\n",
    "#train_data_dir = 'datasets/base_rsna_chexpert/train'\n",
    "#train_data_dir = 'datasets/base_nih_rsna_chexpert/train'\n",
    "\n",
    "#the test directory will be one of the following during varius evaluations:\n",
    "test_data_dir = 'datasets/cropped_ieee_covid19'\n",
    "#test_data_dir = 'datasets/cropped_twitter_covid19'\n",
    "#test_data_dir = 'datasets/cell_pneumonia_224_8/test'\n",
    "\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "channel = 3\n",
    "epochs = 32\n",
    "batch_size = 8 #vary this parameter depending on your GPU capacity\n",
    "input_shape = (img_width, img_height, channel)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% declared data generators, allocate 20% of the training data for validation\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_data_dir, \n",
    "    target_size=(224,224),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\"\"\"test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(224,224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False) #should be used when testing with ground truth cell pneumonia dataset\n",
    "\"\"\"\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(224,224),\n",
    "        batch_size=1,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(val_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(val_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the best performing baseline model vgg-16 in this study, and train it on the augmented train data and evaluate the performance with the different test data used in this study. i.e. ieee covid-19, twitter covid-19 and beseline test data. Repeat the process for each of the augmented combination aforementioned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1_model = applications.VGG16(weights='imagenet', \n",
    "                                   include_top=False,\n",
    "                                   input_shape=(img_width,img_height,channel))\n",
    "feature1_model = Model(inputs=feature1_model.input,\n",
    "                    outputs=feature1_model.get_layer('block5_conv3').output)\n",
    "\n",
    "feature1_model.summary()\n",
    "#addind the top layers\n",
    "x = feature1_model.output\n",
    "x = GlobalAveragePooling2D() (x)\n",
    "predictions = Dense(num_classes, activation='softmax', \n",
    "                    name='predictions')(x)\n",
    "model = Model(inputs=feature1_model.input, \n",
    "              outputs=predictions, name='vgg16_augmented_base_nih_model')\n",
    "model.summary()\n",
    "\n",
    "#enumerate and print layer names\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix optimizer and start training the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "#compile the model\n",
    "model.compile(optimizer=sgd,              \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the generators \n",
    "train_generator.reset()\n",
    "val_generator.reset()\n",
    "\n",
    "# start training\n",
    "start = time.time()\n",
    "filepath = 'weights/' + model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
    "                             save_weights_only=False, \n",
    "                             save_best_only=True, mode='min', period=1)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "custom_vgg16_history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "      epochs=epochs,\n",
    "      validation_data=val_generator,\n",
    "      callbacks=callbacks_list,\n",
    "      class_weight = class_weights,\n",
    "      validation_steps=nb_validation_samples // batch_size + 1, \n",
    "      verbose=1)\n",
    "\n",
    "#print the total time taken for training\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         custom_vgg16_history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"vgg16_augmented_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate with the best stored model weights\n",
    "model.load_weights('weights/vgg16_augmented_model.14-0.9347.h5') #change this to your path and model weights\n",
    "model.summary()\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% when evaluating with the baseline hold out test data\n",
    "#test_generator.reset()\n",
    "#y_pred_val = model.predict_generator(val_generator, \n",
    "#                                                   (nb_validation_samples/batch_size),\n",
    "#                                                    verbose=1)\n",
    "##measure accuracy\n",
    "#vgg16_val_accuracy=accuracy_score(Y_test,y_pred_val.argmax(axis=-1))\n",
    "#print('The accuracy of custom VGG16 model is: ', vgg16_val_accuracy)\n",
    "\n",
    "#predict with Montreal or Twitter COVID-19 test data\n",
    "print('-'*30)\n",
    "print('Predicting on the test data...')\n",
    "print('-'*30)\n",
    "test_generator.reset()\n",
    "y_pred_base = model.predict_generator(test_generator,\n",
    "                                 (nb_test_samples//1),\n",
    "                                 verbose=1)\n",
    "#print prediction shapes\n",
    "print(y_pred_base.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#measure accuracy\n",
    "model_accuracy=accuracy_score(Y_test,y_pred_base.argmax(axis=-1))\n",
    "print('The accuracy of the model is: ', model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification report\n",
    "target_names = ['Bacterial_pneumonia', 'Viral_pneumonia']\n",
    "print(classification_report(Y_test,y_pred_base.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,y_pred_base.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test,y_pred_base.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(15,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=70)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_class_indices has the predicted labels\n",
    "predicted_class_indices=np.argmax(y_pred_base,axis=1)\n",
    "\n",
    "#map the predicted labels with their unique ids \n",
    "#such as filenames to find out what you predicted for which image.\n",
    "\n",
    "labels = (train_generator.class_indices) \n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices] #displays as string\n",
    "\n",
    "#Finally, save the results to a CSV file.\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"ieee_base_nih_chexpert_vgg16.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, record the performance with each of the pooled train collections with respect to the different test data. The prediction performance with the pooled training data should be different from that with the baseline non-augmented train data.\n",
    "we further improve the performance by performing an union of the pateint-IDs with viral pneumonia predictions from each of the predictions from the pooled training data. in that way we found that computing the union of the patient IDs with viral pneumonia predictions obtained by training the model with baseline and (baseline+chexpert) data gave promising performance than\n",
    "other combinations for the IEEE and Twitter COVID-19 data collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization studies: To visualize the learned behavior of the baseline model on the baseline test data and unseen twitter and covid-19 collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = load_model('vgg16_cell_pneumonia_detector.10-0.9282.h5')\n",
    "model.summary()\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True) #optimize to your requirements\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to image to visualize\n",
    "img_path = 'person78_bacteria_380.png'\n",
    "img = image.load_img(img_path)\n",
    "\n",
    "#preprocess the image\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x /= 255 \n",
    "\n",
    "#predict on the image\n",
    "preds = model.predict(x)[0]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin visualization\n",
    "covid_output = model.output[:, 0] #index 1 of the viral pneumonia class, 0 for baterial class; change as per image input\n",
    "\n",
    "#Output feature map from the last convolutional layer\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "\n",
    "#compute the Gradient of the expected class \n",
    "#with regard to the output feature map of block5_conv3 \n",
    "#(or the deepst convolutional layer)\n",
    "grads = K.gradients(covid_output, last_conv_layer.output)[0]\n",
    "\n",
    "#Vector of shape (512,), where each entry is the mean intensity \n",
    "#of the gradient over a specific feature-map channel\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the values of the quantities we defined: \n",
    "#pooled_grads and the output feature map of block5_conv3, given a sample image\n",
    "iterate = K.function([model.input],[pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "#Values of these two quantities, as Numpy arrays, given the sample image\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "\n",
    "#Multiplies each channel in the feature-map array by \n",
    "#“how important this channel is” with regard to the expected class\n",
    "for i in range(512): #number of filters in the deepest convolutional layer\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "#For visualization purposes, we normalize the heatmap between 0 and 1.\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another code peice taken from stack overflow for writng the png output with different DPIs.\n",
    "\n",
    "def writePNGwithdpi(im, filename, dpi=(72,72)):\n",
    "   \"\"\"Save the image as PNG with embedded dpi\"\"\"\n",
    "\n",
    "   # Encode as PNG into memory\n",
    "   retval, buffer = cv2.imencode(\".png\", im)\n",
    "   s = buffer.tostring()\n",
    "\n",
    "   # Find start of IDAT chunk\n",
    "   IDAToffset = s.find(b'IDAT') - 4\n",
    "   pHYs = b'pHYs' + struct.pack('!IIc',int(dpi[0]/0.0254),int(dpi[1]/0.0254),b\"\\x01\" ) \n",
    "   pHYs = struct.pack('!I',9) + pHYs + struct.pack('!I',zlib.crc32(pHYs))\n",
    "   with open(filename, \"wb\") as out:\n",
    "      out.write(buffer[0:IDAToffset])\n",
    "      out.write(pHYs)\n",
    "      out.write(buffer[IDAToffset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using OpenCV to generate an image that superimposes the original image on the heatmap obtained\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "#Resizes the heatmap to be the same size as the original image\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "#Converts the heatmap to RGB \n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "#Applies the heatmap to the original image\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.3 + img #0.4 here is a heatmap intensity factor.\n",
    "\n",
    "#Saves the image to disk\n",
    "#cv2.imwrite('covid_cam_vgg16_fourthimage.jpg', superimposed_img)\n",
    "\n",
    "#if we have to increse the DPI and write to disk\n",
    "writePNGwithdpi(superimposed_img, \"img_gradcam_vis.png\", (300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can use LIME visualization to interpret the models behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image\n",
    "img = image.load_img(img_path)\n",
    "\n",
    "#preprocess the image\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x /= 255 \n",
    "\n",
    "#predict on the image\n",
    "preds = model.predict(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the explainer\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the parameters to modify:\n",
    "hide_color- is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels. Here, we set it to 0 (means gray).\n",
    "top_labels - if not None, ignore labels and produce explanations for the K labels with highest prediction probabilities, where K is this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(x[0], \n",
    "                                         model.predict, top_labels=1, #produces explanations for the top 1 labels\n",
    "                                         hide_color=0, num_samples=42)\n",
    "print(explanation.top_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wall time: more than 3 minutes in local windows system. \n",
    "Explanation: for the top-class. We can see the top 5 superpixels that are most positive towards the class with the rest of the image hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(0, #change the respective class index\n",
    "                                            positive_only=False, \n",
    "                                            num_features=5, hide_rest=False) \n",
    "plt.figure()\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "plt.figure()\n",
    "plt.imshow(x[0] / 2 + 0.5) #this increases te brightness of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
